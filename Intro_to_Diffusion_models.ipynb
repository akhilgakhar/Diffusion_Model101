{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZotkpoA3TJ0Swr7gKGCVK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhilgakhar/Diffusion_Model101/blob/main/Intro_to_Diffusion_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle"
      ],
      "metadata": {
        "id": "osviLYQt_wRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "T73i_Znz_1s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "metadata": {
        "id": "1q2WeB5NAAJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "RHlm-AkZACKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "pCx_oOeFAHPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "TW4WN3y5AJnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d ebrahimelgazar/pixel-art"
      ],
      "metadata": {
        "id": "wyeS-OOMANqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip pixel-art.zip"
      ],
      "metadata": {
        "id": "NyT_HfghAQZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from diffusers import UNet2DModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# pixel art dataset loader.\n",
        "class PixelArtDataset(Dataset):\n",
        "    def __init__(self, images_path, labels_path, transform=None):\n",
        "        self.images = np.load(images_path)\n",
        "        self.labels = np.load(labels_path)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Data Validation (more comprehensive)\n",
        "        if self.images.shape[1:] != (16, 16, 3):  # Check image size (16x16x3)\n",
        "            raise ValueError(f\"Images must be 16x16x3, but are {self.images.shape}\")\n",
        "        if self.labels.ndim != 2 or self.labels.shape[1] != 5:  # Check label shape\n",
        "            raise ValueError(f\"Labels must be a 2D array with 5 elements per row, but are {self.labels.shape}\")\n",
        "        if self.images.shape[0] != self.labels.shape[0]:\n",
        "            raise ValueError(\"Number of images and labels must match\")\n",
        "        if self.images.dtype != np.uint8:  # Check data type. Assuming your images are 8-bit\n",
        "            raise TypeError(f\"Images must be uint8, but are {self.images.dtype}. Convert if necessary\")\n",
        "        if self.labels.dtype != np.float64:  # Check data type. Assuming your labels are float32\n",
        "            raise TypeError(f\"Labels must be float32, but are {self.labels.dtype}. Convert if necessary\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert image to tensor, normalize, and add channel dimension\n",
        "        image = torch.from_numpy(image).float() / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        # No need to unsqueeze for RGB images:\n",
        "        image = image.permute(2, 0, 1) # Change from HxWx3 to 3xHxW (CHW)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = torch.from_numpy(label).float()\n",
        "\n",
        "        return image, label\n",
        "\n",
        "images_path = \"sprites.npy\"\n",
        "labels_path = \"sprites_labels.npy\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "\n",
        "])\n",
        "\n",
        "dataset = PixelArtDataset(images_path, labels_path, transform=transform)\n",
        "\n",
        "# Create a DataLoader for batching and shuffling\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)  # Adjust batch size as needed\n",
        "\n",
        "def corrupt(x, amount):\n",
        "  \"\"\"Corrupt the input `x` by mixing it with noise according to `amount`\"\"\"\n",
        "  noise = torch.rand_like(x)\n",
        "  amount = amount.view(-1, 1, 1, 1) # Sort shape so broadcasting works\n",
        "  return x*(1-amount) + noise*amount\n",
        "\n",
        "# U-Net architecture for the denoising model\n",
        "# Create the network\n",
        "net = UNet2DModel(\n",
        "    sample_size=16,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(32, 64, 64),  # Roughly matching our basic unet example\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"AttnDownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",   # a regular ResNet upsampling block\n",
        "    ),\n",
        ")\n",
        "net.to(device)\n",
        "\n",
        "n_epochs = 10 # Try experimenting\n",
        "batch_size = 2 # Try Experimenting\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "losses = []\n",
        "\n",
        "from tqdm import tqdm # add loss to the leaf of tqdm\n",
        "\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "\n",
        "    for x, y in train_dataloader:\n",
        "\n",
        "        # Get some data and prepare the corrupted version\n",
        "        x = x.to(device) # Data on the GPU\n",
        "        noise_amount = torch.rand(x.shape[0]).to(device) # Pick random noise amounts\n",
        "        noisy_x = corrupt(x, noise_amount) # Create our noisy x\n",
        "\n",
        "        # Get the model prediction\n",
        "        pred = net(noisy_x, 0).sample #<<< Using timestep 0 always, adding .sample\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_fn(pred, x) # How close is the output to the true 'clean' x?\n",
        "\n",
        "        # Backprop and update the params:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # Store the loss for later\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # Print our the average of the loss values for this epoch:\n",
        "    avg_loss = sum(losses[-len(train_dataloader):])/len(train_dataloader)\n",
        "    #print(f'Finished epoch {epoch}. Average loss for this epoch: {avg_loss:05f}')\n",
        "\n",
        "torch.save(net.state_dict(), \"model_state.pth\")\n",
        "\n",
        "# Samples\n",
        "n_steps = 40\n",
        "x = torch.rand(56, 3, 16, 16).to(device)\n",
        "for i in range(n_steps):\n",
        "  noise_amount = torch.ones((x.shape[0], )).to(device) * (1-(i/n_steps)) # Starting high going low\n",
        "  with torch.no_grad():\n",
        "    pred = net(x, 0).sample\n",
        "  mix_factor = 1/(n_steps - i)\n",
        "  x = x*(1-mix_factor) + pred*mix_factor\n",
        "\n",
        "# Plotting\n",
        "fig, axs = plt.subplots(1, 1, figsize=(8, 8))\n",
        "\n",
        "# Convert for display\n",
        "grid_img = torchvision.utils.make_grid(x.detach().cpu(), nrow=10, normalize=True).permute(1, 2, 0).numpy()\n",
        "\n",
        "# Display\n",
        "axs.imshow(grid_img)  # No cmap needed for RGB\n",
        "axs.set_title('Generated Samples')\n",
        "axs.axis(\"off\")\n",
        "\n",
        "plt.savefig('output_image.png')"
      ],
      "metadata": {
        "id": "5AFFRIBDJM_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eyNNDbOkDoQA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}